{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from warcio.archiveiterator import ArchiveIterator\n",
    "import re\n",
    "import requests\n",
    "import sys\n",
    "from pprint import pprint\n",
    "import codecs\n",
    "import urllib.request\n",
    "import gzip\n",
    "import glob\n",
    "import threading\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://commoncrawl.s3.amazonaws.com/crawl-data/CC-MAIN-2020-24/warc.paths.gz\n",
      "http://commoncrawl.s3.amazonaws.com/crawl-data/CC-MAIN-2020-29/warc.paths.gz\n",
      "http://commoncrawl.s3.amazonaws.com/crawl-data/CC-MAIN-2020-34/warc.paths.gz\n",
      "http://commoncrawl.s3.amazonaws.com/crawl-data/CC-MAIN-2020-40/warc.paths.gz\n"
     ]
    }
   ],
   "source": [
    "# beginning months: '05', '10', '16', \n",
    "for x in ['24', '29', '34', '40']:\n",
    "    print(\"http://commoncrawl.s3.amazonaws.com/crawl-data/CC-MAIN-2020-{}/warc.paths.gz\".format(x))\n",
    "    urllib.request.urlretrieve(\"http://commoncrawl.s3.amazonaws.com/crawl-data/CC-MAIN-2020-{}/warc.paths.gz\".format(x), 'paths/{}'.format(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['http://commoncrawl.s3.amazonaws.com/crawl-data/CC-MAIN-2020-40/segments/1600400187354.1/warc/CC-MAIN-20200918061627-20200918091627-00000.warc.gz',\n",
       " 'http://commoncrawl.s3.amazonaws.com/crawl-data/CC-MAIN-2020-40/segments/1600400187354.1/warc/CC-MAIN-20200918061627-20200918091627-00001.warc.gz',\n",
       " 'http://commoncrawl.s3.amazonaws.com/crawl-data/CC-MAIN-2020-40/segments/1600400187354.1/warc/CC-MAIN-20200918061627-20200918091627-00002.warc.gz',\n",
       " 'http://commoncrawl.s3.amazonaws.com/crawl-data/CC-MAIN-2020-40/segments/1600400187354.1/warc/CC-MAIN-20200918061627-20200918091627-00003.warc.gz',\n",
       " 'http://commoncrawl.s3.amazonaws.com/crawl-data/CC-MAIN-2020-40/segments/1600400187354.1/warc/CC-MAIN-20200918061627-20200918091627-00004.warc.gz']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crawl_paths = []\n",
    "for segment_files in glob.glob('paths/*'):\n",
    "    file = gzip.open(segment_files)\n",
    "    urls = [\"http://commoncrawl.s3.amazonaws.com/\" + x for x in str(file.read(), 'utf-8').split('\\n')]\n",
    "    crawl_paths.extend(urls)\n",
    "crawl_paths[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hits(file_name):\n",
    "#     regex = re.compile(\"(\\bcovid|\\becon|\\bcoronavirus)\", re.IGNORECASE)\n",
    "#     regex = re.compile(\"(\\bcovid|\\bcorona)\", re.IGNORECASE)\n",
    "#     regex = re.compile(\n",
    "#         \"(youtu\\.be/|youtube\\.com/(watch\\?(.*\\&)?v=|(embed|v)/))([^?&\\\"'>]+)\"\n",
    "#     )\n",
    "    regex = re.compile('covid', re.IGNORECASE)\n",
    "\n",
    "    entries, matching_entries, hits = 0, 0, 0\n",
    "    stream = requests.get(file_name, stream=True).raw\n",
    "    \n",
    "    records = []\n",
    "    for record in ArchiveIterator(stream):\n",
    "        if record.rec_type == \"warcinfo\":\n",
    "            continue\n",
    "\n",
    "        if not \".com/\" in record.rec_headers.get_header(\"WARC-Target-URI\"):\n",
    "            continue\n",
    "        entries = entries + 1\n",
    "        contents = record.content_stream().read(1000).decode(\"utf-8\", \"replace\")\n",
    "        m = regex.search(contents)\n",
    "        if m:\n",
    "            records.append(record)\n",
    "            print(records)\n",
    "    return records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gather_records(threadName, threadNum, crawl_paths):\n",
    "    print((threadNum, len(crawl_paths), num_threads))\n",
    "    num_warcs_to_read = 2\n",
    "    for i in range(num_warcs_to_read):\n",
    "        path = crawl_paths[i*num_threads + threadNum]\n",
    "        print(\"{} iter {}: {}\\n\".format(threadName, i, len(records) ))\n",
    "        thread_results[threadNum].extend(records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 259604, 8)\n",
      "(1, 259604, 8)\n",
      "(2, 259604, 8)\n",
      "(3, 259604, 8)(4, 259604, 8)\n",
      "(5, 259604, 8)\n",
      "(6, 259604, 8)\n",
      "\n",
      "(7, 259604, 8)\n"
     ]
    }
   ],
   "source": [
    "num_threads = 8\n",
    "threads = []\n",
    "thread_results = [[] for _ in range(num_threads)]\n",
    "for i in range(num_threads):\n",
    "    x = threading.Thread(target=gather_records, args=(\"Thread-{}\".format(i), i, crawl_paths))\n",
    "    threads.append(x)\n",
    "    x.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = []\n",
    "for lst in thread_results:\n",
    "    for record in lst:\n",
    "        if check_criteria(record):\n",
    "            urls.append(get_url(record))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
